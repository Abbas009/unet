{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "Copy of 8.2-deep-dream.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abbas009/unet/blob/master/Copy_of_8_2_deep_dream.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMH2tIyw1mEz",
        "colab_type": "code",
        "outputId": "c0c91623-15db-472d-a846-70a8676dd728",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        }
      },
      "source": [
        "import keras\n",
        "keras.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1TcWcPh1mE-",
        "colab_type": "text"
      },
      "source": [
        "# Deep Dream\n",
        "\n",
        "This notebook contains the code samples found in Chapter 8, Section 2 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n",
        "\n",
        "----\n",
        "\n",
        "[...]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYHnF6Yo1mFA",
        "colab_type": "text"
      },
      "source": [
        "## Implementing Deep Dream in Keras\n",
        "\n",
        "\n",
        "We will start from a convnet pre-trained on ImageNet. In Keras, we have many such convnets available: VGG16, VGG19, Xception, ResNet50... \n",
        "albeit the same process is doable with any of these, your convnet of choice will naturally affect your visualizations, since different \n",
        "convnet architectures result in different learned features. The convnet used in the original Deep Dream release was an Inception model, and \n",
        "in practice Inception is known to produce very nice-looking Deep Dreams, so we will use the InceptionV3 model that comes with Keras.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4TZdpyD1mFC",
        "colab_type": "code",
        "outputId": "550a927e-2d8d-4cb3-e923-ac6c92931680",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "from keras.applications import inception_v3\n",
        "from keras import backend as K\n",
        "\n",
        "# We will not be training our model,\n",
        "# so we use this command to disable all training-specific operations\n",
        "K.set_learning_phase(0)\n",
        "\n",
        "# Build the InceptionV3 network.\n",
        "# The model will be loaded with pre-trained ImageNet weights.\n",
        "model = inception_v3.InceptionV3(weights='imagenet',\n",
        "                                 include_top=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 4s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqWxPqhM1mFH",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Next, we compute the \"loss\", the quantity that we will seek to maximize during the gradient ascent process. In Chapter 5, for filter \n",
        "visualization, we were trying to maximize the value of a specific filter in a specific layer. Here we will simultaneously maximize the \n",
        "activation of all filters in a number of layers. Specifically, we will maximize a weighted sum of the L2 norm of the activations of a \n",
        "set of high-level layers. The exact set of layers we pick (as well as their contribution to the final loss) has a large influence on the \n",
        "visuals that we will be able to produce, so we want to make these parameters easily configurable. Lower layers result in \n",
        "geometric patterns, while higher layers result in visuals in which you can recognize some classes from ImageNet (e.g. birds or dogs).\n",
        "We'll start from a somewhat arbitrary configuration involving four layers -- \n",
        "but you will definitely want to explore many different configurations later on:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWm9opNp1mFJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dict mapping layer names to a coefficient\n",
        "# quantifying how much the layer's activation\n",
        "# will contribute to the loss we will seek to maximize.\n",
        "# Note that these are layer names as they appear\n",
        "# in the built-in InceptionV3 application.\n",
        "# You can list all layer names using `model.summary()`.\n",
        "layer_contributions = {\n",
        "    'mixed2': 0.2,\n",
        "    'mixed3': 3.,\n",
        "    'mixed4': 2.,\n",
        "    'mixed5': 1.5,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0y0tWEx1mFP",
        "colab_type": "text"
      },
      "source": [
        "Now let's define a tensor that contains our loss, i.e. the weighted sum of the L2 norm of the activations of the layers listed above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD7-WWPG1mFR",
        "colab_type": "code",
        "outputId": "5299ab37-7e2b-466e-bff8-a856ed9d3986",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
        "layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
        "\n",
        "# Define the loss.\n",
        "loss = K.variable(0.)\n",
        "for layer_name in layer_contributions:\n",
        "    # Add the L2 norm of the features of a layer to the loss.\n",
        "    coeff = layer_contributions[layer_name]\n",
        "    activation = layer_dict[layer_name].output\n",
        "\n",
        "    # We avoid border artifacts by only involving non-border pixels in the loss.\n",
        "    scaling = K.prod(K.cast(K.shape(activation), 'float32'))\n",
        "    loss += coeff * K.sum(K.square(activation[:, 2: -2, 2: -2, :])) / scaling"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "389GcayT1mFV",
        "colab_type": "text"
      },
      "source": [
        "Now we can set up the gradient ascent process:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktcEtHw-1mFX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This holds our generated image\n",
        "dream = model.input\n",
        "\n",
        "# Compute the gradients of the dream with regard to the loss.\n",
        "grads = K.gradients(loss, dream)[0]\n",
        "\n",
        "# Normalize gradients.\n",
        "grads /= K.maximum(K.mean(K.abs(grads)), 1e-7)\n",
        "\n",
        "# Set up function to retrieve the value\n",
        "# of the loss and gradients given an input image.\n",
        "outputs = [loss, grads]\n",
        "fetch_loss_and_grads = K.function([dream], outputs)\n",
        "\n",
        "def eval_loss_and_grads(x):\n",
        "    outs = fetch_loss_and_grads([x])\n",
        "    loss_value = outs[0]\n",
        "    grad_values = outs[1]\n",
        "    return loss_value, grad_values\n",
        "\n",
        "def gradient_ascent(x, iterations, step, max_loss=None):\n",
        "    for i in range(iterations):\n",
        "        loss_value, grad_values = eval_loss_and_grads(x)\n",
        "        if max_loss is not None and loss_value > max_loss:\n",
        "            break\n",
        "        print('...Loss value at', i, ':', loss_value)\n",
        "        x += step * grad_values\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-OQetmg1mFm",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Finally, here is the actual Deep Dream algorithm.\n",
        "\n",
        "First, we define a list of \"scales\" (also called \"octaves\") at which we will process the images. Each successive scale is larger than \n",
        "previous one by a factor 1.4 (i.e. 40% larger): we start by processing a small image and we increasingly upscale it:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHD-zTrw1mFp",
        "colab_type": "text"
      },
      "source": [
        "![deep dream process](https://s3.amazonaws.com/book.keras.io/img/ch8/deepdream_process.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oLYQ_6F1mFr",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Then, for each successive scale, from the smallest to the largest, we run gradient ascent to maximize the loss we have previously defined, \n",
        "at that scale. After each gradient ascent run, we upscale the resulting image by 40%.\n",
        "\n",
        "To avoid losing a lot of image detail after each successive upscaling (resulting in increasingly blurry or pixelated images), we leverage a \n",
        "simple trick: after each upscaling, we reinject the lost details back into the image, which is possible since we know what the original \n",
        "image should look like at the larger scale. Given a small image S and a larger image size L, we can compute the difference between the \n",
        "original image (assumed larger than L) resized to size L and the original resized to size S -- this difference quantifies the details lost \n",
        "when going from S to L."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kBCU6_01mFt",
        "colab_type": "text"
      },
      "source": [
        "The code above below leverages the following straightforward auxiliary Numpy functions, which all do just as their name suggests. They \n",
        "require to have SciPy installed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHxu5i5m1mFv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy\n",
        "from keras.preprocessing import image\n",
        "\n",
        "def resize_img(img, size):\n",
        "    img = np.copy(img)\n",
        "    factors = (1,\n",
        "               float(size[0]) / img.shape[1],\n",
        "               float(size[1]) / img.shape[2],\n",
        "               1)\n",
        "    return scipy.ndimage.zoom(img, factors, order=1)\n",
        "\n",
        "\n",
        "def save_img(img, fname):\n",
        "    pil_img = deprocess_image(np.copy(img))\n",
        "    scipy.misc.imsave(fname, pil_img)\n",
        "\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    # Util function to open, resize and format pictures\n",
        "    # into appropriate tensors.\n",
        "    img = image.load_img(image_path)\n",
        "    img = image.img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = inception_v3.preprocess_input(img)\n",
        "    return img\n",
        "\n",
        "\n",
        "def deprocess_image(x):\n",
        "    # Util function to convert a tensor into a valid image.\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        x = x.reshape((3, x.shape[2], x.shape[3]))\n",
        "        x = x.transpose((1, 2, 0))\n",
        "    else:\n",
        "        x = x.reshape((x.shape[1], x.shape[2], 3))\n",
        "    x /= 2.\n",
        "    x += 0.5\n",
        "    x *= 255.\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GgB_j5y1mF0",
        "colab_type": "code",
        "outputId": "260f8463-054c-4cfd-e36f-3ff1e34e39b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Playing with these hyperparameters will also allow you to achieve new effects\n",
        "\n",
        "step = 0.01  # Gradient ascent step size\n",
        "num_octave = 3  # Number of scales at which to run gradient ascent\n",
        "octave_scale = 1.4  # Size ratio between scales\n",
        "iterations = 20  # Number of ascent steps per scale\n",
        "\n",
        "# If our loss gets larger than 10,\n",
        "# we will interrupt the gradient ascent process, to avoid ugly artifacts\n",
        "max_loss = 10.\n",
        "\n",
        "# Fill this to the path to the image you want to use\n",
        "base_image_path = \"F:\\\\Advance_deep_learning\\\\BT_Neuro_MRI2_MEDIMG-PHO_EN.jpg\"\n",
        "\n",
        "# Load the image into a Numpy array\n",
        "img = preprocess_image(base_image_path)\n",
        "\n",
        "# We prepare a list of shape tuples\n",
        "# defining the different scales at which we will run gradient ascent\n",
        "original_shape = img.shape[1:3]\n",
        "successive_shapes = [original_shape]\n",
        "for i in range(1, num_octave):\n",
        "    shape = tuple([int(dim / (octave_scale ** i)) for dim in original_shape])\n",
        "    successive_shapes.append(shape)\n",
        "\n",
        "# Reverse list of shapes, so that they are in increasing order\n",
        "successive_shapes = successive_shapes[::-1]\n",
        "\n",
        "# Resize the Numpy array of the image to our smallest scale\n",
        "original_img = np.copy(img)\n",
        "shrunk_original_img = resize_img(img, successive_shapes[0])\n",
        "\n",
        "for shape in successive_shapes:\n",
        "    print('Processing image shape', shape)\n",
        "    img = resize_img(img, shape)\n",
        "    img = gradient_ascent(img,\n",
        "                          iterations=iterations,\n",
        "                          step=step,\n",
        "                          max_loss=max_loss)\n",
        "    upscaled_shrunk_original_img = resize_img(shrunk_original_img, shape)\n",
        "    same_size_original = resize_img(original_img, shape)\n",
        "    lost_detail = same_size_original - upscaled_shrunk_original_img\n",
        "\n",
        "    img += lost_detail\n",
        "    shrunk_original_img = resize_img(original_img, shape)\n",
        "    save_img(img, fname='dream_at_scale_' + str(shape) + '.png')\n",
        "\n",
        "save_img(img, fname='final_dream.png')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-fe3dcb59e041>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Load the image into a Numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_image_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# We prepare a list of shape tuples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-ce5ad8dff637>\u001b[0m in \u001b[0;36mpreprocess_image\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Util function to open, resize and format pictures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# into appropriate tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    108\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[1;32m    109\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'L'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2530\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2531\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'F:\\\\Advance_deep_learning\\\\BT_Neuro_MRI2_MEDIMG-PHO_EN.jpg'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voAqVfWh1mF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.imshow(deprocess_image(np.copy(img)))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}